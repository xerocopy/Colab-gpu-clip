{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMzL5Abrv3Wm"
   },
   "source": [
    "# **1) Download all dependencies for A.I. Animator**\n",
    "\n",
    "\n",
    "**Uses Vq+C to generate the frames and a series of animation techniques on top of it to zoom, rotate, and shift by x, y pixels**\n",
    "\n",
    "**Referece Video link: https://www.youtube.com/watch?v=WS-cnyTd9Dw&t=208s\n",
    "\n",
    "@title **Licensed under the MIT License**\n",
    "\n",
    "Copyright (c) 2021 Katherine Crowson\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in\n",
    "all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "THE SOFTWARE.\n",
    "\n",
    "@update: this copy of software is downloaded by Jingchao Song and edited at 08/11/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posix\n",
      "Python 3.8.10\n",
      "Tue Nov 15 03:42:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.60.02    Driver Version: 512.15       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   32C    P8    32W / 340W |   1280MiB / 10240MiB |      5%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "GPU 0: NVIDIA GeForce RTX 3080 (UUID: GPU-da42ea4e-3f3d-fbc0-a706-418abb0de72b)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# return nt means the notebook is run on win, posix means linux\n",
    "print(os.name)\n",
    "# check the enviornment and hardware at local machine\n",
    "# check python version\n",
    "!python --version\n",
    "# hardware\n",
    "!nvidia-smi\n",
    "#check GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oA9isL44gb5V"
   },
   "outputs": [],
   "source": [
    "working_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install dependencies in colab env\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngVYOAZEghkK",
    "outputId": "125f7d32-5c3d-47a2-af7b-68ef5d907056",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CLIP...\n",
      "fatal: destination path 'CLIP' already exists and is not an empty directory.\n",
      "Downloading Python AI libraries...\n",
      "fatal: destination path 'taming-transformers' already exists and is not an empty directory.\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy) (0.2.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (2022.10.31)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.8/dist-packages (2.2.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.8/dist-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf) (6.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pytorch-lightning==1.6.5 in /usr/local/lib/python3.8/dist-packages (1.6.5)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (1.23.4)\n",
      "Collecting torch>=1.8.*\n",
      "  Using cached torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (2.11.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (3.19.6)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (4.64.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (0.10.2)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (4.4.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (21.3)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (6.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.5) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.5) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.5) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.5) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.5) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (65.5.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.50.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.34.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.5) (3.0.9)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\" in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (3.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio>=1.24.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (5.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.10.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.5) (3.2.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.5) (2.8)\n",
      "Installing collected packages: torch\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "torchaudio 0.9.0 requires torch==1.9.0, but you'll have torch 1.13.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.13.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: kornia in /usr/local/lib/python3.8/dist-packages (0.6.8)\n",
      "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from kornia) (1.13.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.1->kornia) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.1->kornia) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.1->kornia) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.1->kornia) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.1->kornia) (8.5.0.96)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->kornia) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->kornia) (65.5.1)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->kornia) (0.34.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Installing libraries for handling metadata...\n",
      "Requirement already satisfied: stegano in /usr/local/lib/python3.8/dist-packages (0.10.2)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=4.5.4 in /usr/local/lib/python3.8/dist-packages (from stegano) (4.6.0.66)\n",
      "Requirement already satisfied: piexif<2.0.0,>=1.1.3 in /usr/local/lib/python3.8/dist-packages (from stegano) (1.1.3)\n",
      "Requirement already satisfied: crayons<0.5.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from stegano) (0.4.0)\n",
      "Requirement already satisfied: pillow<10.0.0,>=9.0.0 in /usr/local/lib/python3.8/dist-packages (from stegano) (9.3.0)\n",
      "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in /usr/local/lib/python3.8/dist-packages (from opencv-python<5.0.0,>=4.5.4->stegano) (1.23.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from crayons<0.5.0,>=0.4.0->stegano) (0.4.6)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement exempi (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for exempi\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: python-xmp-toolkit in /usr/local/lib/python3.8/dist-packages (2.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from python-xmp-toolkit) (2022.6)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: imgtag in /usr/local/lib/python3.8/dist-packages (1.1.6)\n",
      "Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.8/dist-packages (from imgtag) (5.9.4)\n",
      "Requirement already satisfied: python-xmp-toolkit>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from imgtag) (2.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from python-xmp-toolkit>=2.0.1->imgtag) (2022.6)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pillow==9.3.0 in /usr/local/lib/python3.8/dist-packages (9.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Installing Python video creation libraries...\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.8/dist-packages (0.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Installing extra dependencies...\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.17.3; python_version >= \"3.8\" in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (0.29.32)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\n",
      "Found existing installation: torch 1.13.0\n",
      "Uninstalling torch-1.13.0:\n",
      "  Successfully uninstalled torch-1.13.0\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'pytorch-nightly'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (2.22.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/dist-packages (from imageio) (9.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio) (1.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Installation finished!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# @title #**Library installation**\n",
    "# @markdown This cell will take a while because you have to download multiple libraries\n",
    "\n",
    "print(\"Downloading CLIP...\")\n",
    "!git clone https://github.com/openai/CLIP                \n",
    " \n",
    "print(\"Downloading Python AI libraries...\")\n",
    "!git clone https://github.com/CompVis/taming-transformers \n",
    "!pip install ftfy            \n",
    "!pip install regex \n",
    "!pip install tqdm \n",
    "!pip install omegaconf \n",
    "!pip install pytorch-lightning==1.6.5 \n",
    "!pip install kornia                                       \n",
    "!pip install einops                                       \n",
    " \n",
    "print(\"Installing libraries for handling metadata...\")\n",
    "!pip install stegano                                      \n",
    "!pip install exempi # installed in cli\n",
    "#@ file:///exempi-2.5.2.tar.bz2\n",
    "#https://libopenraw.freedesktop.org/download/exempi-2.5.2.tar.bz2\n",
    "\n",
    "!pip install python-xmp-toolkit                           \n",
    "!pip install imgtag          \n",
    "#!pip install pillow==7.1.2          \n",
    "#!pip install pillow==8.4.0 \n",
    "!pip install pillow==9.3.0\n",
    "\n",
    "print(\"Installing Python video creation libraries...\")\n",
    "!pip install imageio-ffmpeg \n",
    "path = f'{working_dir}/steps'\n",
    "!mkdir --parents {path}\n",
    "\n",
    "print(\"Installing extra dependencies...\")\n",
    "!pip install opencv-python\n",
    "!pip install cython\n",
    "# !pip uninstall torchvision -y\n",
    "# !pip uninstall torch -y\n",
    "# !pip uninstall torch -y  # yes twice\n",
    "# !pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "!pip install imageio\n",
    "!pip install pandas\n",
    "\n",
    "print(\"Installation finished!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchvision 0.11.2+rocm4.2\n",
      "Uninstalling torchvision-0.11.2+rocm4.2:\n",
      "  Successfully uninstalled torchvision-0.11.2+rocm4.2\n",
      "Found existing installation: torch 1.10.1+rocm4.2\n",
      "Uninstalling torch-1.10.1+rocm4.2:\n",
      "  Successfully uninstalled torch-1.10.1+rocm4.2\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.1+cu113\n",
      "  Using cached https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp38-cp38-linux_x86_64.whl (1821.4 MB)\n",
      "Collecting torchvision==0.11.2\n",
      "  Using cached https://download.pytorch.org/whl/rocm4.2/torchvision-0.11.2%2Brocm4.2-cp38-cp38-linux_x86_64.whl (67.0 MB)\n",
      "Requirement already up-to-date: torchaudio==0.10.1 in /usr/local/lib/python3.8/dist-packages (0.10.1+rocm4.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2) (9.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2) (1.23.4)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.10.1+cu113 torchvision-0.11.2+rocm4.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchvision -y\n",
    "!pip uninstall torch -y\n",
    "!pip uninstall torch -y  # yes twice\n",
    "\n",
    "#!pip install -U torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "!pip install -U torch==1.10.1+cu113 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#this above version uses cudatoolkit==11.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 3.8.10 (default, Jun 22 2022, 20:18:18) \n",
      "[GCC 9.4.0]\n",
      "B 1.10.1+cu113\n",
      "C True\n",
      "D True\n",
      "E _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080', major=8, minor=6, total_memory=10239MB, multi_processor_count=68)\n",
      "F tensor([1., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "print('A', sys.version)\n",
    "print('B', torch.__version__)\n",
    "print('C', torch.cuda.is_available())\n",
    "print('D', torch.backends.cudnn.enabled)\n",
    "device = torch.device('cuda')\n",
    "print('E', torch.cuda.get_device_properties(device))\n",
    "print('F', torch.tensor([1.0, 2.0]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "xkgbrCZqgkFw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Resuming transfer from byte position 692\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "** Resuming transfer from byte position 980092370\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "#@title #**Selection of models to download**\n",
    "#@markdown By default, the notebook downloads Model 16384 from ImageNet. There are others such as ImageNet 1024, COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are not downloaded by default, since it would be in vain if you are not going to use them, so if you want to use them, simply select the models to download.\n",
    "\n",
    "imagenet_1024 = False #@param {type:\"boolean\"}\n",
    "imagenet_16384 = True #@param {type:\"boolean\"}\n",
    "coco = False #@param {type:\"boolean\"}\n",
    "faceshq = False #@param {type:\"boolean\"}\n",
    "wikiart_16384 = False #@param {type:\"boolean\"}\n",
    "sflckr = False #@param {type:\"boolean\"}\n",
    "\n",
    "if imagenet_1024:\n",
    "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
    "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
    "if imagenet_16384:\n",
    "    !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
    "    !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
    "if coco:\n",
    "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
    "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
    "if faceshq:\n",
    "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
    "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
    "if wikiart_16384:\n",
    "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' #WikiArt 16384\n",
    "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' #WikiArt 16384\n",
    "if sflckr:\n",
    "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
    "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "rXx8O5Fvgm6v",
    "outputId": "26260592-cb91-4586-9f88-21385f7f47cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-fe8fafeed45d>\", line 35, in <module>\n",
      "    import kornia.augmentation as K\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/__init__.py\", line 8, in <module>\n",
      "    from . import augmentation, color, contrib, core, enhance, feature, io, losses, metrics, morphology, tracking, utils, x\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/augmentation/__init__.py\", line 1, in <module>\n",
      "    from kornia.augmentation._2d import (\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/augmentation/_2d/__init__.py\", line 3, in <module>\n",
      "    from kornia.augmentation._2d.mix import *\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/augmentation/_2d/mix/__init__.py\", line 1, in <module>\n",
      "    from kornia.augmentation._2d.mix.cutmix import RandomCutMix, RandomCutMixV2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/augmentation/_2d/mix/cutmix.py\", line 7, in <module>\n",
      "    from kornia.augmentation._2d.mix.base import MixAugmentationBase, MixAugmentationBaseV2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/augmentation/_2d/mix/base.py\", line 10, in <module>\n",
      "    from kornia.geometry.boxes import Boxes\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kornia/geometry/boxes.py\", line 582, in <module>\n",
      "    class Boxes3D:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_script.py\", line 1294, in script\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_recursive.py\", line 44, in _compile_and_register_class\n",
      "    script_class = torch._C._jit_script_class_compile(qualified_name, ast, defaults, rcb)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_recursive.py\", line 838, in try_compile_fn\n",
      "    \"Python functions or methods currently.\\n\"\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_script.py\", line 1310, in script\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_recursive.py\", line 838, in try_compile_fn\n",
      "    \"Python functions or methods currently.\\n\"\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_script.py\", line 1310, in script\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_recursive.py\", line 838, in try_compile_fn\n",
      "    \"Python functions or methods currently.\\n\"\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_script.py\", line 1310, in script\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_recursive.py\", line 838, in try_compile_fn\n",
      "    \"Python functions or methods currently.\\n\"\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/jit/_script.py\", line 1310, in script\n",
      "RuntimeError: !seen_default_arg || arg.kwarg_only()INTERNAL ASSERT FAILED at \"../aten/src/ATen/core/function_schema.h\":238, please report a bug to PyTorch. Non-default positional argument follows default argument. Parameter padding_mode in _pad(Tensor input, Tensor grid, str mode=\"constant\", str padding_mode, bool? align_corners) -> (Tensor)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 720, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 667, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 646, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# @title #**Loading of libraries and definitions**\n",
    " \n",
    "import argparse\n",
    "import math\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    " \n",
    "sys.path.append('./taming-transformers')\n",
    "\n",
    "# Some models include transformers, others need explicit pip install\n",
    "try:\n",
    "    import transformers\n",
    "except Exception:\n",
    "    !pip install transformers\n",
    "    import transformers\n",
    "\n",
    "from IPython import display\n",
    "from base64 import b64encode\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from taming.models import cond_transformer, vqgan\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    " \n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import ImageFile, Image\n",
    "from imgtag import ImgTag    # metadata \n",
    "from libxmp import *         # metadata\n",
    "import libxmp                # metadata\n",
    "from stegano import lsb\n",
    "import json\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    " \n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    " \n",
    " \n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    " \n",
    " \n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    " \n",
    " \n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    " \n",
    "    input = input.view([n * c, 1, h, w])\n",
    " \n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    " \n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    " \n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    " \n",
    " \n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    " \n",
    " \n",
    "replace_grad = ReplaceGrad.apply\n",
    " \n",
    " \n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    " \n",
    " \n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    " \n",
    " \n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    " \n",
    " \n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    " \n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    " \n",
    " \n",
    "def parse_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    " \n",
    " \n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
    "            K.RandomSharpness(0.3,p=0.4),\n",
    "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
    "            K.RandomPerspective(0.2,p=0.4),\n",
    "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
    "        self.noise_fac = 0.1\n",
    " \n",
    " \n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    " \n",
    " \n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    " \n",
    " \n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNicbSEk7aA-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsjcbIZa7BMi"
   },
   "source": [
    "#**2) Setting Up the A.I. Animator**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTdit0n41GDg"
   },
   "source": [
    "Tik tok resolution: 340px by 570px\n",
    "\n",
    "Nice Square resolution: 500px by 500px or 400px by 400px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "cellView": "form",
    "id": "9SqvStcBgr_0"
   },
   "outputs": [],
   "source": [
    "#@title **Parameters for Model, 12 frames=1 second**\n",
    "\n",
    "width =  340#@param {type:\"number\"}\n",
    "height =  570#@param {type:\"number\"}\n",
    "model = \"vqgan_imagenet_f16_16384\"\n",
    "interval =  1\n",
    "initial_image = \"\"\n",
    "target_images = \"\"\n",
    "seed = -1\n",
    "max_frames = 300#@param {type:\"number\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cellView": "form",
    "id": "5nAUcLvHnLE3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The width and height you have specified may be too high, in which case you will encounter out of memory errors either at the image generation stage or the video synthesis stage. If so, try reducing the resolution\n"
     ]
    }
   ],
   "source": [
    "#@title **Parameters for Animating**\n",
    "text_prompts = \"0:(fantasy world matte painting splashes of colors in unreal engine: 1)\" #@param {type:\"string\"}\n",
    "key_frames = True\n",
    "angle = \"0: (0)\"#@param {type:\"string\"}\n",
    "zoom = \"0:(0.95)\"#@param {type:\"string\"}\n",
    "translation_x = \"0:(0)\"#@param {type:\"string\"}\n",
    "translation_y = \"0:(5)\"#@param {type:\"string\"}\n",
    "iterations_per_frame = \"0: (10)\"\n",
    "save_all_iterations = False\n",
    "if initial_image != \"\":\n",
    "    print(\n",
    "        \"WARNING: You have specified an initial image. Note that the image resolution \"\n",
    "        \"will be inherited from this image, not whatever width and height you specified. \"\n",
    "        \"If the initial image resolution is too high, this can result in out of memory errors.\"\n",
    "    )\n",
    "elif width * height > 160000:\n",
    "    print(\n",
    "        \"WARNING: The width and height you have specified may be too high, in which case \"\n",
    "        \"you will encounter out of memory errors either at the image generation stage or the \"\n",
    "        \"video synthesis stage. If so, try reducing the resolution\"\n",
    "    )\n",
    "model_names={\n",
    "    \"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n",
    "    \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
    "    \"wikiart_1024\":\"WikiArt 1024\",\n",
    "    \"wikiart_16384\":\"WikiArt 16384\",\n",
    "    \"coco\":\"COCO-Stuff\",\n",
    "    \"faceshq\":\"FacesHQ\",\n",
    "    \"sflckr\":\"S-FLCKR\"\n",
    "}\n",
    "model_name = model_names[model]\n",
    "\n",
    "if seed == -1:\n",
    "    seed = None\n",
    "\n",
    "def parse_key_frames(string, prompt_parser=None):\n",
    "    import re\n",
    "    pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
    "    frames = dict()\n",
    "    for match_object in re.finditer(pattern, string):\n",
    "        frame = int(match_object.groupdict()['frame'])\n",
    "        param = match_object.groupdict()['param']\n",
    "        if prompt_parser:\n",
    "            frames[frame] = prompt_parser(param)\n",
    "        else:\n",
    "            frames[frame] = param\n",
    "\n",
    "    if frames == {} and len(string) != 0:\n",
    "        raise RuntimeError('Key Frame string not correctly formatted')\n",
    "    return frames\n",
    "\n",
    "def get_inbetweens(key_frames, integer=False):\n",
    "    key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
    "    for i, value in key_frames.items():\n",
    "        key_frame_series[i] = value\n",
    "    key_frame_series = key_frame_series.astype(float)\n",
    "    key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
    "    if integer:\n",
    "        return key_frame_series.astype(int)\n",
    "    return key_frame_series\n",
    "\n",
    "def split_key_frame_text_prompts(frames):\n",
    "    prompt_dict = dict()\n",
    "    for i, parameters in frames.items():\n",
    "        prompts = parameters.split('|')\n",
    "        for prompt in prompts:\n",
    "            string, value = prompt.split(':')\n",
    "            string = string.strip()\n",
    "            value = float(value.strip())\n",
    "            if string in prompt_dict:\n",
    "                prompt_dict[string][i] = value\n",
    "            else:\n",
    "                prompt_dict[string] = {i: value}\n",
    "    prompt_series_dict = dict()\n",
    "    for prompt, values in prompt_dict.items():\n",
    "        value_string = (\n",
    "            ', '.join([f'{value}: ({values[value]})' for value in values])\n",
    "        )\n",
    "        prompt_series = get_inbetweens(parse_key_frames(value_string))\n",
    "        prompt_series_dict[prompt] = prompt_series\n",
    "    prompt_list = []\n",
    "    for i in range(max_frames):\n",
    "        prompt_list.append(\n",
    "            ' | '.join(\n",
    "                [f'{prompt}: {prompt_series_dict[prompt][i]}'\n",
    "                 for prompt in prompt_series_dict]\n",
    "            )\n",
    "        )\n",
    "    return prompt_list\n",
    "\n",
    "if key_frames:\n",
    "    try:\n",
    "        text_prompts_series = split_key_frame_text_prompts(\n",
    "            parse_key_frames(text_prompts)\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `text_prompts` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `text_prompts` as \"\n",
    "            f'\"0: ({text_prompts}:1)\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        text_prompts = f\"0: ({text_prompts}:1)\"\n",
    "        text_prompts_series = split_key_frame_text_prompts(\n",
    "            parse_key_frames(text_prompts)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        target_images_series = split_key_frame_text_prompts(\n",
    "            parse_key_frames(target_images)\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `target_images` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `target_images` as \"\n",
    "            f'\"0: ({target_images}:1)\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        target_images = f\"0: ({target_images}:1)\"\n",
    "        target_images_series = split_key_frame_text_prompts(\n",
    "            parse_key_frames(target_images)\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `angle` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `angle` as \"\n",
    "            f'\"0: ({angle})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        angle = f\"0: ({angle})\"\n",
    "        angle_series = get_inbetweens(parse_key_frames(angle))\n",
    "\n",
    "    try:\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `zoom` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `zoom` as \"\n",
    "            f'\"0: ({zoom})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        zoom = f\"0: ({zoom})\"\n",
    "        zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
    "\n",
    "    try:\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_x` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_x` as \"\n",
    "            f'\"0: ({translation_x})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_x = f\"0: ({translation_x})\"\n",
    "        translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
    "\n",
    "    try:\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `translation_y` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `translation_y` as \"\n",
    "            f'\"0: ({translation_y})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        translation_y = f\"0: ({translation_y})\"\n",
    "        translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
    "\n",
    "    try:\n",
    "        iterations_per_frame_series = get_inbetweens(\n",
    "            parse_key_frames(iterations_per_frame), integer=True\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            \"WARNING: You have selected to use key frames, but you have not \"\n",
    "            \"formatted `iterations_per_frame` correctly for key frames.\\n\"\n",
    "            \"Attempting to interpret `iterations_per_frame` as \"\n",
    "            f'\"0: ({iterations_per_frame})\"\\n'\n",
    "            \"Please read the instructions to find out how to use key frames \"\n",
    "            \"correctly.\\n\"\n",
    "        )\n",
    "        iterations_per_frame = f\"0: ({iterations_per_frame})\"\n",
    "        \n",
    "        iterations_per_frame_series = get_inbetweens(\n",
    "            parse_key_frames(iterations_per_frame), integer=True\n",
    "        )\n",
    "else:\n",
    "    text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
    "    if text_prompts == ['']:\n",
    "        text_prompts = []\n",
    "    if target_images == \"None\" or not target_images:\n",
    "        target_images = []\n",
    "    else:\n",
    "        target_images = target_images.split(\"|\")\n",
    "        target_images = [image.strip() for image in target_images]\n",
    "\n",
    "    angle = float(angle)\n",
    "    zoom = float(zoom)\n",
    "    translation_x = float(translation_x)\n",
    "    translation_y = float(translation_y)\n",
    "    iterations_per_frame = int(iterations_per_frame)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    prompts=text_prompts,\n",
    "    image_prompts=target_images,\n",
    "    noise_prompt_seeds=[],\n",
    "    noise_prompt_weights=[],\n",
    "    size=[width, height],\n",
    "    init_weight=0.,\n",
    "    clip_model='ViT-B/32',\n",
    "    vqgan_config=f'{model}.yaml',\n",
    "    vqgan_checkpoint=f'{model}.ckpt',\n",
    "    step_size=0.1,\n",
    "    cutn=64,\n",
    "    cut_pow=1.,\n",
    "    display_freq=interval,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68hVsk_K7Y3j"
   },
   "source": [
    "# 3) **Running the A.I. Animator**\n",
    "\n",
    "You can stop whenever you want by pressing the stop button! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tensorflow-tutorials', 'exempi-2.6.2', 'taming', 'vqgan_imagenet_f16_16384.yaml', 'boost_1_80_0', 'taming-transformers', '.ipynb_checkpoints', 'Copy3_of_AI_Animating_Beginner_Notebook.ipynb', 'CLIP', 'vqgan_imagenet_f16_16384.ckpt', 'requirements.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "path = f'{working_dir}/steps'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(path)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(path)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellView": "form",
    "id": "owUFbInCHW9W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to clear Accounted PIDs for GPU 00000000:09:00.0: Insufficient Permissions\n",
      "Terminating early due to previous errors.\n",
      "Using device: cuda:0\n",
      "Using seed: 6980560163342874347\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from vqgan_imagenet_f16_16384.ckpt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [75], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m toksX, toksY \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m f, args\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m f\n\u001b[1;32m     64\u001b[0m sideX, sideY \u001b[38;5;241m=\u001b[39m toksX \u001b[38;5;241m*\u001b[39m f, toksY \u001b[38;5;241m*\u001b[39m f\n\u001b[0;32m---> 65\u001b[0m z_min \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     66\u001b[0m z_max \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mquantize\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     67\u001b[0m stop_on_next_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "#@title #**Fire up the A.I**\n",
    "\n",
    "# #linux remove dir\n",
    "# !rm -r {path}\n",
    "# #linex make a new dir\n",
    "# !mkdir --parents {path}\n",
    "\n",
    "\n",
    "if key_frames:\n",
    "    filename = \"video.mp4\"\n",
    "else:\n",
    "    filename = f\"{'_'.join(text_prompts).replace(' ', '')}.mp4\"\n",
    "filepath = f'{working_dir}/{filename}'\n",
    "\n",
    "\n",
    "if key_frames:\n",
    "    filename = \"video.mp4\"\n",
    "else:\n",
    "    filename = f\"{'_'.join(text_prompts).replace(' ', '')}.mp4\"\n",
    "filepath = f'{working_dir}/{filename}'\n",
    "\n",
    "# Delete memory from previous runs\n",
    "!nvidia-smi -caa\n",
    "for var in ['device', 'model', 'perceptor', 'z']:\n",
    "  try:\n",
    "      del globals()[var]\n",
    "  except:\n",
    "      pass\n",
    "\n",
    "try:\n",
    "    import gc\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if not key_frames:\n",
    "    if text_prompts:\n",
    "        print('Using text prompts:', text_prompts)\n",
    "    if target_images:\n",
    "        print('Using image prompts:', target_images)\n",
    "if args.seed is None:\n",
    "    seed = torch.seed()\n",
    "else:\n",
    "    seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "print('Using seed:', seed)\n",
    " \n",
    "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
    "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    " \n",
    "cut_size = perceptor.visual.input_resolution\n",
    "e_dim = model.quantize.e_dim\n",
    "f = 2**(model.decoder.num_resolutions - 1)\n",
    "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
    "n_toks = model.quantize.n_e\n",
    "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
    "sideX, sideY = toksX * f, toksY * f\n",
    "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
    "\n",
    "def read_image_workaround(path):\n",
    "    \"\"\"OpenCV reads images as BGR, Pillow saves them as RGB. Work around\n",
    "    this incompatibility to avoid colour inversions.\"\"\"\n",
    "    im_tmp = cv2.imread(path)\n",
    "    return cv2.cvtColor(im_tmp, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i in range(max_frames):\n",
    "    if stop_on_next_loop:\n",
    "      break\n",
    "    if key_frames:\n",
    "        text_prompts = text_prompts_series[i]\n",
    "        text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
    "        if text_prompts == ['']:\n",
    "            text_prompts = []\n",
    "        args.prompts = text_prompts\n",
    "\n",
    "        target_images = target_images_series[i]\n",
    "\n",
    "        if target_images == \"None\" or not target_images:\n",
    "            target_images = []\n",
    "        else:\n",
    "            target_images = target_images.split(\"|\")\n",
    "            target_images = [image.strip() for image in target_images]\n",
    "        args.image_prompts = target_images\n",
    "\n",
    "        angle = angle_series[i]\n",
    "        zoom = zoom_series[i]\n",
    "        translation_x = translation_x_series[i]\n",
    "        translation_x = -1 * translation_x\n",
    "        translation_y = translation_y_series[i]\n",
    "        iterations_per_frame = iterations_per_frame_series[i]\n",
    "        print(\n",
    "            f'text_prompts: {text_prompts}'\n",
    "            f'angle: {angle}',\n",
    "            f'zoom: {zoom}',\n",
    "            f'translation_x: {translation_x}',\n",
    "            f'translation_y: {translation_y}',\n",
    "            f'iterations_per_frame: {iterations_per_frame}'\n",
    "        )\n",
    "    try:\n",
    "        if i == 0 and initial_image != \"\":\n",
    "            img_0 = read_image_workaround(initial_image)\n",
    "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
    "        elif i == 0 and not os.path.isfile(f'{working_dir}/steps/{i:04d}.png'):\n",
    "            one_hot = F.one_hot(\n",
    "                torch.randint(n_toks, [toksY * toksX], device=device), n_toks\n",
    "            ).float()\n",
    "            z = one_hot @ model.quantize.embedding.weight\n",
    "            z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            if save_all_iterations:\n",
    "                img_0 = read_image_workaround(\n",
    "                    f'{working_dir}/steps/{i:04d}_{iterations_per_frame}.png')\n",
    "            else:\n",
    "                img_0 = read_image_workaround(f'{working_dir}/steps/{i:04d}.png')\n",
    "\n",
    "            center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
    "            trans_mat = np.float32(\n",
    "                [[1, 0, translation_x],\n",
    "                [0, 1, translation_y]]\n",
    "            )\n",
    "            rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
    "\n",
    "            trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
    "            rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
    "            transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
    "\n",
    "            img_0 = cv2.warpPerspective(\n",
    "                img_0,\n",
    "                transformation_matrix,\n",
    "                (img_0.shape[1], img_0.shape[0]),\n",
    "                borderMode=cv2.BORDER_WRAP\n",
    "            )\n",
    "            z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
    "        i += 1\n",
    "\n",
    "        z_orig = z.clone()\n",
    "        z.requires_grad_(True)\n",
    "        opt = optim.Adam([z], lr=args.step_size)\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "        pMs = []\n",
    "\n",
    "        for prompt in args.prompts:\n",
    "            txt, weight, stop = parse_prompt(prompt)\n",
    "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
    "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "        for prompt in args.image_prompts:\n",
    "            path, weight, stop = parse_prompt(prompt)\n",
    "            img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
    "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "            embed = perceptor.encode_image(normalize(batch)).float()\n",
    "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "        for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
    "            gen = torch.Generator().manual_seed(seed)\n",
    "            embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
    "            pMs.append(Prompt(embed, weight).to(device))\n",
    "\n",
    "        def synth(z):\n",
    "            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "            return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
    "\n",
    "        def add_xmp_data(filename):\n",
    "            imagen = ImgTag(filename=filename)\n",
    "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            if args.prompts:\n",
    "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            else:\n",
    "                imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
    "            imagen.close()\n",
    "\n",
    "        def add_stegano_data(filename):\n",
    "            data = {\n",
    "                \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
    "                \"notebook\": \"VQGAN+CLIP\",\n",
    "                \"i\": i,\n",
    "                \"model\": model_name,\n",
    "                \"seed\": str(seed),\n",
    "            }\n",
    "            lsb.hide(filename, json.dumps(data)).save(filename)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def checkin(i, losses):\n",
    "            losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "            tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "            out = synth(z)\n",
    "            TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
    "            add_stegano_data('progress.png')\n",
    "            add_xmp_data('progress.png')\n",
    "            display.display(display.Image('progress.png'))\n",
    "\n",
    "        def save_output(i, img, suffix=None):\n",
    "            filename = \\\n",
    "                f\"{working_dir}/steps/{i:04}{'_' + suffix if suffix else ''}.png\"\n",
    "            imageio.imwrite(filename, np.array(img))\n",
    "            add_stegano_data(filename)\n",
    "            add_xmp_data(filename)\n",
    "\n",
    "        def ascend_txt(i, save=True, suffix=None):\n",
    "            out = synth(z)\n",
    "            iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "\n",
    "            result = []\n",
    "\n",
    "            if args.init_weight:\n",
    "                result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
    "\n",
    "            for prompt in pMs:\n",
    "                result.append(prompt(iii))\n",
    "            img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            if save:\n",
    "                save_output(i, img, suffix=suffix)\n",
    "            return result\n",
    "\n",
    "        def train(i, save=True, suffix=None):\n",
    "            opt.zero_grad()\n",
    "            lossAll = ascend_txt(i, save=save, suffix=suffix)\n",
    "            if i % args.display_freq == 0 and save:\n",
    "                checkin(i, lossAll)\n",
    "            loss = sum(lossAll)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                z.copy_(z.maximum(z_min).minimum(z_max))\n",
    "\n",
    "        with tqdm() as pbar:\n",
    "            if iterations_per_frame == 0:\n",
    "                save_output(i, img_0)\n",
    "            j = 1\n",
    "            while True:\n",
    "                suffix = (str(j) if save_all_iterations else None)\n",
    "                if j >= iterations_per_frame:\n",
    "                    train(i, save=True, suffix=suffix)\n",
    "                    break\n",
    "                if save_all_iterations:\n",
    "                    train(i, save=True, suffix=suffix)\n",
    "                else:\n",
    "                    train(i, save=False, suffix=suffix)\n",
    "                j += 1\n",
    "                pbar.update()\n",
    "    except KeyboardInterrupt:\n",
    "      stop_on_next_loop = True\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T_qhdtW-Epi"
   },
   "source": [
    "# 4) **View Animation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellView": "form",
    "id": "c3GaYef_g6oK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m      6\u001b[0m init_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m last_frame \u001b[38;5;241m=\u001b[39m \u001b[43mi\u001b[49m\n\u001b[1;32m      8\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m     10\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "# @title ### **Create video**\n",
    "\n",
    "# import subprocess in case this cell is run without the above cells\n",
    "import subprocess\n",
    "\n",
    "init_frame = 1\n",
    "last_frame = i\n",
    "fps = 12\n",
    "\n",
    "frames = []\n",
    "# tqdm.write('Generating video...')\n",
    "try:\n",
    "    zoomed\n",
    "except NameError:\n",
    "    image_path = f'{working_dir}/steps/%04d.png'\n",
    "else:\n",
    "    image_path = f'{working_dir}/steps/zoomed_%04d.png'\n",
    "\n",
    "cmd = [\n",
    "    'ffmpeg',\n",
    "    '-y',\n",
    "    '-vcodec',\n",
    "    'png',\n",
    "    '-r',\n",
    "    str(fps),\n",
    "    '-start_number',\n",
    "    str(init_frame),\n",
    "    '-i',\n",
    "    image_path,\n",
    "    '-c:v',\n",
    "    'libx264',\n",
    "    '-vf',\n",
    "    f'fps={fps}',\n",
    "    '-pix_fmt',\n",
    "    'yuv420p',\n",
    "    '-crf',\n",
    "    '17',\n",
    "    '-preset',\n",
    "    'veryslow',\n",
    "    filepath\n",
    "]\n",
    "\n",
    "process = subprocess.Popen(cmd, cwd=f'{working_dir}/steps/', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "if process.returncode != 0:\n",
    "    print(stderr)\n",
    "    print(\n",
    "        \"You may be able to avoid this error by backing up the frames,\"\n",
    "        \"restarting the notebook, and running only the video synthesis cells,\"\n",
    "        \"or by decreasing the resolution of the image generation steps. \"\n",
    "        \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
    "        \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
    "        \"If these steps do not work, please post the traceback in the github.\"\n",
    "    )\n",
    "    raise RuntimeError(stderr)\n",
    "else:\n",
    "    print(\"The video is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "n_j6qdIIg6oS"
   },
   "outputs": [],
   "source": [
    "# @title **See video in the browser**\n",
    "# @markdown This process may take a little longer. If you don't want to wait, download it by executing the next cell instead of using this cell.\n",
    "mp4 = open(filepath,'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "display.HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "mpA629ZOg6oT"
   },
   "outputs": [],
   "source": [
    "# @title **Download video**\n",
    "from google.colab import files\n",
    "files.download(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UvfjxCGHwz2"
   },
   "source": [
    "# Optional: Super-Slomo for smoothing movement\n",
    "\n",
    "This step might run out of memory if you run it right after the steps above. If it does, restart the notebook, upload a saved copy of the video from the previous step (or get it from google drive) and define the variable `filepath` with the path to the video before running the cells below again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An5SykYz5mWK"
   },
   "outputs": [],
   "source": [
    "# filepath = \"/content/download (10).mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "q9cCek0HHzDw"
   },
   "outputs": [],
   "source": [
    "# @title **Download Super-Slomo model**\n",
    "!git clone -q --depth 1 https://github.com/avinashpaliwal/Super-SloMo.git\n",
    "from os.path import exists\n",
    "def download_from_google_drive(file_id, file_name):\n",
    "  # download a file from the Google Drive link\n",
    "  !rm -f ./cookie\n",
    "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
    "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
    "  confirm_text = confirm_text[0]\n",
    "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
    "  \n",
    "pretrained_model = 'SuperSloMo.ckpt'\n",
    "if not exists(pretrained_model):\n",
    "  download_from_google_drive('1IvobLDbRiBgZr3ryCRrWL8xDbMZ-KnpF', pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ybgyF2flcmME"
   },
   "outputs": [],
   "source": [
    "# import subprocess in case this cell is run without the above cells\n",
    "import subprocess\n",
    "\n",
    "SLOW_MOTION_FACTOR = 2#@param {type:\"number\"}\n",
    "TARGET_FPS = 12#@param {type:\"number\"}\n",
    "\n",
    "cmd1 = [\n",
    "    'python',\n",
    "    'Super-SloMo/video_to_slomo.py',\n",
    "    '--checkpoint',\n",
    "    pretrained_model,\n",
    "    '--video',\n",
    "    filepath,\n",
    "    '--sf',\n",
    "    str(SLOW_MOTION_FACTOR),\n",
    "    '--fps',\n",
    "    str(TARGET_FPS),\n",
    "    '--output',\n",
    "    f'{filepath}-slomo.mkv',\n",
    "]\n",
    "process = subprocess.Popen(cmd1, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "if process.returncode != 0:\n",
    "    raise RuntimeError(stderr)\n",
    "\n",
    "cmd2 = [\n",
    "    'ffmpeg',\n",
    "    '-i',\n",
    "    f'{filepath}-slomo.mkv',\n",
    "    '-pix_fmt',\n",
    "    'yuv420p',\n",
    "    '-crf',\n",
    "    '17',\n",
    "    '-preset',\n",
    "    'veryslow',\n",
    "    f'{filepath}-slomo.mp4',\n",
    "]\n",
    "\n",
    "process = subprocess.Popen(cmd2, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "if process.returncode != 0:\n",
    "    raise RuntimeError(stderr)\n",
    "    print(stderr)\n",
    "    print(\n",
    "        \"You may be able to avoid this error by backing up the frames,\"\n",
    "        \"restarting the notebook, and running only the video synthesis cells,\"\n",
    "        \"or by decreasing the resolution of the image generation steps. \"\n",
    "        \"If you restart the notebook, you will have to define the `filepath` manually\"\n",
    "        \"by adding `filepath = 'PATH_TO_THE_VIDEO'` to the beginning of this cell. \"\n",
    "        \"If these steps do not work, please post the traceback in the github.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FwQCcrggH1oY"
   },
   "outputs": [],
   "source": [
    "# @title **See video in the browser**\n",
    "# @markdown This process may take a little longer. If you don't want to wait, download it by executing the next cell instead of using this cell.\n",
    "from base64 import b64encode\n",
    "from IPython import display\n",
    "mp4 = open(f'{filepath}-slomo.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "display.HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WfJrASI3ctpP"
   },
   "outputs": [],
   "source": [
    "# @title **Download video**\n",
    "from google.colab import files\n",
    "files.download(f'{filepath}-slomo.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqZI0zN5gEOb"
   },
   "source": [
    "**Intermediate A.I. Animating Notebook!**\n",
    "\n",
    "https://colab.research.google.com/drive/1U3s-qQlYcRvMidIP-0dcM4FyNrtJkJgC?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAyr89Kw-mpQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AMzL5Abrv3Wm"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
